{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkContext('local[*]')\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[], functions=[sum(id#0L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *HashAggregate(keys=[], functions=[partial_sum(id#0L)])\n",
      "      +- *Filter (id#0L > 100)\n",
      "         +- *Range (0, 1000, step=1, splits=Some(2))\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000).filter(\"id > 100\").selectExpr(\"sum(id)\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 s, sys: 110 ms, total: 37.8 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def make_measurements(input_id):\n",
    "    n_measurements = np.random.poisson(15)\n",
    "    meas_values = input_id + np.random.randn(n_measurements)\n",
    "    return zip(n_measurements*[input_id], meas_values.tolist())\n",
    "    \n",
    "rdd = sc.range(100000).flatMap(make_measurements).collect()\n",
    "meas_table = spark.createDataFrame(rdd, schema=(\"obj_id\", \"meas_value\"))\n",
    "meas_table.registerTempTable(\"meas_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1503414"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meas_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary_table = spark.sql(\"SELECT obj_id, count(*) as n_epochs, \"\n",
    "                             \"min(meas_value) as min_val, max(meas_value) as max_val \"\n",
    "                             \"FROM meas_table GROUP BY obj_id\")\n",
    "summary_table.registerTempTable(\"summary_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>min_val</th>\n",
       "      <th>max_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.961687</td>\n",
       "      <td>1.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>5.064148</td>\n",
       "      <td>9.762524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>5.193034</td>\n",
       "      <td>7.944696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6.324873</td>\n",
       "      <td>10.110992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4.038313</td>\n",
       "      <td>6.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.193034</td>\n",
       "      <td>2.944696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.954731</td>\n",
       "      <td>4.158932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>6.954731</td>\n",
       "      <td>9.158932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.064148</td>\n",
       "      <td>4.762524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.324873</td>\n",
       "      <td>5.110992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_id  n_epochs   min_val    max_val\n",
       "0       0        17 -0.961687   1.668768\n",
       "1       7        23  5.064148   9.762524\n",
       "2       6        15  5.193034   7.944696\n",
       "3       9        12  6.324873  10.110992\n",
       "4       5        17  4.038313   6.668768\n",
       "5       1        15  0.193034   2.944696\n",
       "6       3         9  1.954731   4.158932\n",
       "7       8         9  6.954731   9.158932\n",
       "8       2        23  0.064148   4.762524\n",
       "9       4        12  1.324873   5.110992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1353020\n",
      "CPU times: user 20 ms, sys: 10 ms, total: 30 ms\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targetObjects = spark.sql(\"SELECT summary_table.obj_id, meas_value FROM  meas_table \"\n",
    "                          \"JOIN summary_table ON (summary_table.obj_id = meas_table.obj_id) \"\n",
    "                          \"WHERE summary_table.min_val > 10000\")\n",
    "print(targetObjects.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [obj_id#443L, meas_value#272]\n",
      "+- *SortMergeJoin [obj_id#271L], [obj_id#443L], Inner\n",
      "   :- *Sort [obj_id#271L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(obj_id#271L, 200)\n",
      "   :     +- *Filter isnotnull(obj_id#271L)\n",
      "   :        +- Scan ExistingRDD[obj_id#271L,meas_value#272]\n",
      "   +- *Sort [obj_id#443L ASC NULLS FIRST], false, 0\n",
      "      +- *Project [obj_id#443L]\n",
      "         +- *Filter (isnotnull(min_val#287) && (min_val#287 > 10000.0))\n",
      "            +- *HashAggregate(keys=[obj_id#443L], functions=[min(meas_value#444)])\n",
      "               +- Exchange hashpartitioning(obj_id#443L, 200)\n",
      "                  +- *HashAggregate(keys=[obj_id#443L], functions=[partial_min(meas_value#444)])\n",
      "                     +- *Filter isnotnull(obj_id#443L)\n",
      "                        +- Scan ExistingRDD[obj_id#443L,meas_value#444]\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT summary_table.obj_id, meas_value FROM  meas_table \"\n",
    "                          \"JOIN summary_table ON (summary_table.obj_id = meas_table.obj_id) \"\n",
    "                          \"WHERE summary_table.min_val > 10000\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 370 ms, sys: 20 ms, total: 390 ms\n",
      "Wall time: 4.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = targetObjects.groupBy(\"obj_id\").mean(\"meas_value\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89998"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
