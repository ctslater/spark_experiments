{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = SparkContext('local[*]')\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*HashAggregate(keys=[], functions=[sum(id#0L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *HashAggregate(keys=[], functions=[partial_sum(id#0L)])\n",
      "      +- *Filter (id#0L > 100)\n",
      "         +- *Range (0, 1000, step=1, splits=Some(2))\n"
     ]
    }
   ],
   "source": [
    "spark.range(1000).filter(\"id > 100\").selectExpr(\"sum(id)\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51 s, sys: 150 ms, total: 51.1 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def make_measurements(input_id):\n",
    "    n_measurements = np.random.poisson(15)\n",
    "    meas_values = input_id + np.random.randn(n_measurements)\n",
    "    htm_id = int(200*np.random.random())\n",
    "    return zip(n_measurements*[input_id], n_measurements*[htm_id], meas_values.tolist())\n",
    "    \n",
    "rdd = sc.range(100000).flatMap(make_measurements).collect()\n",
    "meas_table = spark.createDataFrame(rdd, schema=(\"obj_id\", \"htm_id\", \"meas_value\"))\n",
    "meas_table.registerTempTable(\"meas_table\")\n",
    "meas_table.write.parquet(\"meas_table.parquet\", partitionBy=['htm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1499400"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meas_table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 30 ms, total: 40 ms\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "summary_table = spark.sql(\"SELECT obj_id, first(htm_id) as htm_id, count(*) as n_epochs, \"\n",
    "                             \"min(meas_value) as min_val, max(meas_value) as max_val \"\n",
    "                             \"FROM meas_table GROUP BY obj_id\")\n",
    "summary_table.registerTempTable(\"summary_table\")\n",
    "summary_table.write.parquet(\"summary_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_id</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>min_val</th>\n",
       "      <th>max_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.961687</td>\n",
       "      <td>1.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>5.064148</td>\n",
       "      <td>9.762524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>5.193034</td>\n",
       "      <td>7.944696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>6.324873</td>\n",
       "      <td>10.110992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>4.038313</td>\n",
       "      <td>6.668768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0.193034</td>\n",
       "      <td>2.944696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1.954731</td>\n",
       "      <td>4.158932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>6.954731</td>\n",
       "      <td>9.158932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0.064148</td>\n",
       "      <td>4.762524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1.324873</td>\n",
       "      <td>5.110992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_id  n_epochs   min_val    max_val\n",
       "0       0        17 -0.961687   1.668768\n",
       "1       7        23  5.064148   9.762524\n",
       "2       6        15  5.193034   7.944696\n",
       "3       9        12  6.324873  10.110992\n",
       "4       5        17  4.038313   6.668768\n",
       "5       1        15  0.193034   2.944696\n",
       "6       3         9  1.954731   4.158932\n",
       "7       8         9  6.954731   9.158932\n",
       "8       2        23  0.064148   4.762524\n",
       "9       4        12  1.324873   5.110992"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete and Reload\n",
    "-----------------\n",
    "\n",
    "If we leave the original RDDs around, subsequent queries can cheat and reuse the aggregation relations that built the summary table. Since we want to force actual joins between two independent tables, we need to delete the RDDs and reload them from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meas_table.unpersist()\n",
    "summary_table.unpersist()\n",
    "spark.sql(\"DROP TABLE summary_table\")\n",
    "spark.sql(\"DROP TABLE meas_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_table = spark.read.parquet(\"summary_table.parquet\")\n",
    "summary_table.registerTempTable(\"summary_table\")\n",
    "\n",
    "meas_table = spark.read.parquet(\"meas_table.parquet\")\n",
    "meas_table.registerTempTable(\"meas_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['summary_table.obj_id, 'meas_value]\n",
      "+- 'Filter ('summary_table.min_val > 10000)\n",
      "   +- 'Join Inner, ('summary_table.obj_id = 'meas_table.obj_id)\n",
      "      :- 'UnresolvedRelation `meas_table`\n",
      "      +- 'UnresolvedRelation `summary_table`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "obj_id: bigint, meas_value: double\n",
      "Project [obj_id#728L, meas_value#741]\n",
      "+- Filter (min_val#731 > cast(10000 as double))\n",
      "   +- Join Inner, (obj_id#728L = obj_id#740L)\n",
      "      :- SubqueryAlias meas_table, `meas_table`\n",
      "      :  +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "      +- SubqueryAlias summary_table, `summary_table`\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [obj_id#728L, meas_value#741]\n",
      "+- Join Inner, (obj_id#728L = obj_id#740L)\n",
      "   :- Project [obj_id#740L, meas_value#741]\n",
      "   :  +- Filter isnotnull(obj_id#740L)\n",
      "   :     +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "   +- Project [obj_id#728L]\n",
      "      +- Filter ((isnotnull(min_val#731) && (min_val#731 > 10000.0)) && isnotnull(obj_id#728L))\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [obj_id#728L, meas_value#741]\n",
      "+- *BroadcastHashJoin [obj_id#740L], [obj_id#728L], Inner, BuildRight\n",
      "   :- *Project [obj_id#740L, meas_value#741]\n",
      "   :  +- *Filter isnotnull(obj_id#740L)\n",
      "   :     +- *FileScan parquet [obj_id#740L,meas_value#741,htm_id#742] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/meas_table.parquet], PartitionCount: 200, PartitionFilters: [], PushedFilters: [IsNotNull(obj_id)], ReadSchema: struct<obj_id:bigint,meas_value:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "      +- *Project [obj_id#728L]\n",
      "         +- *Filter ((isnotnull(min_val#731) && (min_val#731 > 10000.0)) && isnotnull(obj_id#728L))\n",
      "            +- *FileScan parquet [obj_id#728L,min_val#731] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/summary_table.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(min_val), GreaterThan(min_val,10000.0), IsNotNull(obj_id)], ReadSchema: struct<obj_id:bigint,min_val:double>\n",
      "1349603\n",
      "CPU times: user 0 ns, sys: 10 ms, total: 10 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = (\"SELECT summary_table.obj_id, meas_value FROM  meas_table \"\n",
    "         \"JOIN summary_table ON (summary_table.obj_id = meas_table.obj_id) \"\n",
    "         \"WHERE summary_table.min_val > 10000\")\n",
    "spark.sql(query).explain(True)\n",
    "targetObjects = spark.sql(query)\n",
    "print(targetObjects.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 ms, sys: 0 ns, total: 10 ms\n",
      "Wall time: 19.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = summary_table.groupBy(\"htm_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['summary_table.obj_id, 'meas_value]\n",
      "+- 'Filter ('summary_table.obj_id = 5)\n",
      "   +- 'Join Inner, ('summary_table.obj_id = 'meas_table.obj_id)\n",
      "      :- 'UnresolvedRelation `meas_table`\n",
      "      +- 'UnresolvedRelation `summary_table`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "obj_id: bigint, meas_value: double\n",
      "Project [obj_id#728L, meas_value#741]\n",
      "+- Filter (obj_id#728L = cast(5 as bigint))\n",
      "   +- Join Inner, (obj_id#728L = obj_id#740L)\n",
      "      :- SubqueryAlias meas_table, `meas_table`\n",
      "      :  +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "      +- SubqueryAlias summary_table, `summary_table`\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [obj_id#728L, meas_value#741]\n",
      "+- Join Inner, (obj_id#728L = obj_id#740L)\n",
      "   :- Project [obj_id#740L, meas_value#741]\n",
      "   :  +- Filter ((obj_id#740L = 5) && isnotnull(obj_id#740L))\n",
      "   :     +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "   +- Project [obj_id#728L]\n",
      "      +- Filter (isnotnull(obj_id#728L) && (obj_id#728L = 5))\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [obj_id#728L, meas_value#741]\n",
      "+- *BroadcastHashJoin [obj_id#740L], [obj_id#728L], Inner, BuildRight\n",
      "   :- *Project [obj_id#740L, meas_value#741]\n",
      "   :  +- *Filter ((obj_id#740L = 5) && isnotnull(obj_id#740L))\n",
      "   :     +- *FileScan parquet [obj_id#740L,meas_value#741,htm_id#742] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/meas_table.parquet], PartitionCount: 200, PartitionFilters: [], PushedFilters: [EqualTo(obj_id,5), IsNotNull(obj_id)], ReadSchema: struct<obj_id:bigint,meas_value:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n",
      "      +- *Project [obj_id#728L]\n",
      "         +- *Filter (isnotnull(obj_id#728L) && (obj_id#728L = 5))\n",
      "            +- *FileScan parquet [obj_id#728L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/summary_table.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(obj_id), EqualTo(obj_id,5)], ReadSchema: struct<obj_id:bigint>\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 57.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "query = (\"SELECT summary_table.obj_id, meas_value FROM  meas_table \"\n",
    "         \"JOIN summary_table ON (summary_table.obj_id = meas_table.obj_id) \"\n",
    "         \"WHERE summary_table.obj_id = 5\")\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(obj_id=39, htm_id=86, n_epochs=16, min_val=37.575302509516334, max_val=40.51324216407764)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_table.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project ['meas_table.obj_id, 'meas_value]\n",
      "+- 'Filter ('summary_table.obj_id = 5)\n",
      "   +- 'Join Inner, (('summary_table.obj_id = 'meas_table.obj_id) && ('summary_table.htm_id = 'meas_table.htm_id))\n",
      "      :- 'UnresolvedRelation `meas_table`\n",
      "      +- 'UnresolvedRelation `summary_table`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "obj_id: bigint, meas_value: double\n",
      "Project [obj_id#740L, meas_value#741]\n",
      "+- Filter (obj_id#728L = cast(5 as bigint))\n",
      "   +- Join Inner, ((obj_id#728L = obj_id#740L) && (htm_id#729L = cast(htm_id#742 as bigint)))\n",
      "      :- SubqueryAlias meas_table, `meas_table`\n",
      "      :  +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "      +- SubqueryAlias summary_table, `summary_table`\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [obj_id#740L, meas_value#741]\n",
      "+- Join Inner, ((obj_id#728L = obj_id#740L) && (htm_id#729L = cast(htm_id#742 as bigint)))\n",
      "   :- Filter (((obj_id#740L = 5) && isnotnull(htm_id#742)) && isnotnull(obj_id#740L))\n",
      "   :  +- Relation[obj_id#740L,meas_value#741,htm_id#742] parquet\n",
      "   +- Project [obj_id#728L, htm_id#729L]\n",
      "      +- Filter ((isnotnull(obj_id#728L) && (obj_id#728L = 5)) && isnotnull(htm_id#729L))\n",
      "         +- Relation[obj_id#728L,htm_id#729L,n_epochs#730L,min_val#731,max_val#732] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "*Project [obj_id#740L, meas_value#741]\n",
      "+- *BroadcastHashJoin [obj_id#740L, cast(htm_id#742 as bigint)], [obj_id#728L, htm_id#729L], Inner, BuildRight\n",
      "   :- *Project [obj_id#740L, meas_value#741, htm_id#742]\n",
      "   :  +- *Filter ((obj_id#740L = 5) && isnotnull(obj_id#740L))\n",
      "   :     +- *FileScan parquet [obj_id#740L,meas_value#741,htm_id#742] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/meas_table.parquet], PartitionCount: 200, PartitionFilters: [isnotnull(htm_id#742)], PushedFilters: [EqualTo(obj_id,5), IsNotNull(obj_id)], ReadSchema: struct<obj_id:bigint,meas_value:double>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true], input[1, bigint, true]))\n",
      "      +- *Project [obj_id#728L, htm_id#729L]\n",
      "         +- *Filter ((isnotnull(obj_id#728L) && (obj_id#728L = 5)) && isnotnull(htm_id#729L))\n",
      "            +- *FileScan parquet [obj_id#728L,htm_id#729L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/jovyan/work/data/summary_table.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(obj_id), EqualTo(obj_id,5), IsNotNull(htm_id)], ReadSchema: struct<obj_id:bigint,htm_id:bigint>\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 59.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# AND summary_table.htm_id = meas_table.htm_id\n",
    "query = (\"SELECT meas_table.obj_id, meas_value FROM  meas_table \"\n",
    "         \"JOIN summary_table ON (summary_table.obj_id = meas_table.obj_id AND summary_table.htm_id = meas_table.htm_id) \"\n",
    "         \"WHERE summary_table.obj_id = 5\")\n",
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
